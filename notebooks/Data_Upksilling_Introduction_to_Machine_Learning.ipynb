{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Data Upskilling Learning Club: Introduction to Machine Learning.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XPPwsb_36BR"
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/adelnehme/data-upskilling-learning-club-IV/blob/master/assets/dc_amazon_logo.png?raw=True\" alt = \"DataCamp icon\" width=\"50%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "## **Data UpSkilling Learning Club: Introduction to Machine Learning with Python**\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Key Session Takeaways**\n",
        "\n",
        "- Understand the different types of machine learning and when to use them\n",
        "- Know when machine learning is applicable and when it’s not\n",
        "- A discussion on the different components of the machine learning workflow\n",
        "- Apply a simple supervised learning workflow to classify customer churn\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **The Dataset**\n",
        "\n",
        "The dataset to be used in this session is a CSV file named `telco.csv`, which contains data on telecom customers churning and some of their key behaviors. It contains the following columns:\n",
        "\n",
        "**Features**:\n",
        "\n",
        "- `customerID`: Unique identifier of a customer.\n",
        "- `gender`: Gender of customer.\n",
        "- `SeniorCitizen`: Binary variable indicating if customer is senior citizen.\n",
        "- `Partner`: Binary variable if customer has a partner.\n",
        "- `Dependents`: Binary variable if customer has dependent.\n",
        "- `tenure`: Number of weeks as a customer.\n",
        "- `PhoneService`: Whether customer has phone service.\n",
        "- `MultipleLines`: Whether customer has multiple lines.\n",
        "- `InternetService`: What type of internet service customer has (`\"DSL\"`, `\"Fiber optic\"`, `\"No\"`).\n",
        "- `OnlineSecurity`: Whether customer has online security service.\n",
        "- `OnlineBackup`: Whether customer has online backup service.\n",
        "- `DeviceProtection`: Whether customer has device protection service.\n",
        "- `TechSupport`: Whether customer has tech support service.\n",
        "- `StreamingTV`: Whether customer has TV streaming service.\n",
        "- `StreamingMovies`: Whether customer has movies streaming service.\n",
        "- `Contract`: Customer Contract Type (`'Month-to-month'`, `'One year'`, `'Two year'`).\n",
        "- `PaperlessBilling`: Whether paperless billing is enabled.\n",
        "- `PaymentMethod`: Payment method.\n",
        "- `MonthlyCharges`: Amount of monthly charges in $.\n",
        "- `TotalCharges`: Amount of total charges so far.\n",
        "\n",
        "**Target Variable**:\n",
        "\n",
        "- `Churn`: Whether customer `'Stayed'` or `'Churned'`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r88DIBQXVshr"
      },
      "source": [
        "## **Data Import & Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUZGmwco35DZ"
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB84Bnxj0ivD"
      },
      "source": [
        "# Read in dataset\n",
        "telco = pd.read_csv('https://raw.githubusercontent.com/adelnehme/machine-learning-with-scikit-learn-live-training/master/data/telco_churn.csv')\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1xRb-Ar0ivJ"
      },
      "source": [
        "# Print header\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsmNCE8U0iva"
      },
      "source": [
        "# Print info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IWmBMGQ9yqi"
      },
      "source": [
        "The **null model** is a model of reference to use for classification accuracy - where the  **null accuracy** is the accuracy of the model if we always choose the most frequent class *(or outcome)*. Accuracy is determined here by the following:\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "$$\\large{accuracy = \\frac{\\# \\space times \\space model \\space is \\space right}{total \\space number \\space of \\space predictions}}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnasvjxF0iv9"
      },
      "source": [
        "# Find the null model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyebp5IvAdGo"
      },
      "source": [
        "$$\\large{null \\space accuracy = \\frac{\\# \\space times \\space model \\space predicted \\space \"Stayed\"}{total \\space number \\space of \\space predictions}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd6E2YbQ0iwB"
      },
      "source": [
        "# Find the null model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYynPBTc-vYU"
      },
      "source": [
        "In this particular instance, the null model (always predicting `\"Stayed\"`) is 73.4% - and any meaningful model that improves performance will have to break that accuracy score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqLtk5Et0iwN"
      },
      "source": [
        "# Drop customer ID column\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8k7xK95M1Ro"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 1</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0yg1rYr0iws"
      },
      "source": [
        "## **Exploratory Analysis for Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFCOeFKaFdb8"
      },
      "source": [
        "In order to understand which features have predictive power, which variables to use in feature engineering, and to build a common sense understanding of what is driving churn to understand results, it essential to explore the data and observe how **features** interact with the **target** variable. To make this analysis convenient, we will isolate features between:\n",
        "\n",
        "- Numeric columns _(e.g., Age)_\n",
        "- Categorical columns _(e.g., Marriage Status)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmxBKliY0iw0"
      },
      "source": [
        "# Grab a look at the header\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-QiBIJopLDd"
      },
      "source": [
        "#### **Isolating categorical and numeric column names**\n",
        "\n",
        "We can get all the data types of a DataFrame using the `.dtypes` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hWyT3ncpq55"
      },
      "source": [
        "# Get dtypes of column\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvVbZZarYbq8"
      },
      "source": [
        "We can extract columns names from a DataFrame using the `.columns` attribute, and extract columns with specific data types using the `.select_dtypes()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3znf0qLD0iw2"
      },
      "source": [
        "# Get all feature names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoVBM6plRQDS"
      },
      "source": [
        "# Get categorical and numeric column names\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etRPlz5mrpnL"
      },
      "source": [
        "> #### **Data Visualization Refresher** \n",
        "> \n",
        "> A `matplotlib` visualization is made of 3 components:\n",
        "> - A **figure** which houses in one or many subplots (or axes).\n",
        "> - The **axes** objects ~ the subplots within the figure.\n",
        "> - The plot inside each subplot or axes.\n",
        ">\n",
        "> We can generate a figure with subplots using the following function:\n",
        ">\n",
        "> `fig, axes = plt.subplots(nrow, ncol)`\n",
        "> \n",
        "> <p align=\"left\">\n",
        "<img src=\"https://github.com/adelnehme/intro-to-data-visualization-Python-live-training/blob/master/images/subplots.gif?raw=true\" width=\"55%\">\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kvRhH6VvSYP"
      },
      "source": [
        "#### **Visualizing target variable (Churn) relationship with categorical features (columns)**\n",
        "\n",
        "To visualize the count of different categorical values by `Churn`, we can use the `sns.countplot(x, hue, data, ax)` function which takes in:\n",
        "- `x`: The column name being counted.\n",
        "- `hue`: The column name used for grouping the data.\n",
        "- `data`: The DataFrame being visualized.\n",
        "- `ax`: Which axes in the figure to assign the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz-uBHw50iw-"
      },
      "source": [
        "# Setting aesthetics for better viewing\n",
        "plt.rcParams[\"axes.labelsize\"] = 5\n",
        "sns.set(font_scale=5) \n",
        "\n",
        "# Create figure and axes\n",
        "fig, axes = plt.subplots(5, 3, figsize = (100, 100))\n",
        "\n",
        "# Iterate over each axes, and plot a countplot with categorical columns\n",
        "for ax, column in zip(axes.flatten(), categorical_names):\n",
        "    \n",
        "    # Create countplot\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ7wyQbrvOG7"
      },
      "source": [
        "#### **Visualizing target variable relationship with continuous features**\n",
        "\n",
        "A great way to observe the differences between two groups (or categories) of data according to a numeric value is a boxplot, which visualizes the following:\n",
        "\n",
        "<p align=\"left\">\n",
        "<img src=\"https://github.com/adelnehme/intro-to-data-visualization-Python-live-training/blob/master/images/boxplot.png?raw=true\" alt = \"DataCamp icon\" width=\"80%\">\n",
        "</p>\n",
        "\n",
        "It can be visusalized as such:\n",
        "\n",
        "- `sns.boxplot(x=, y=, data=)`\n",
        "  - `x`: Categorical variable we want to group our data by.\n",
        "  - `y`: Numeric variable being observed by group.\n",
        "  - `data`: The DataFrame being used.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTQU6my40ixF"
      },
      "source": [
        "# Setting aesthetics for better viewing\n",
        "plt.rcParams[\"axes.labelsize\"] = 1\n",
        "sns.set(font_scale=1) \n",
        " \n",
        "# Create figure and axes\n",
        "fig, axes = plt.subplots(1, 3, figsize = (20, 8))\n",
        "\n",
        "# Iterate over each axes, and plot a boxplot with numeric columns\n",
        "for ax, column in zip(axes.flatten(), numeric_names):\n",
        "    \n",
        "    # Create a boxplot\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwY6BYxn6osc"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 2</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP69JRvx0ixL"
      },
      "source": [
        "## **Data pre-processing for machine learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMQWcTP1Y8rd"
      },
      "source": [
        "Many machine learning algorithms require data to be processed before being passed into an algorithm first - dependent on whether data is numeric or categorical, the processing strategy is different.\n",
        "\n",
        "\n",
        "**Train-test split**\n",
        "\n",
        "Since we have one DataFrame that is fully labelled, we want to evaluate accuracy on unseen data. To do so, we can split our data into Training data, i.e. the data that is trained on a Machine Learning algorithm with its labels, and test data, the data treated as \"unseen data\" used to evaluate the accuracy of our model.\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/adelnehme/data-upskilling-learning-club-IV/blob/master/assets/telco.png?raw=true\" width=\"68%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfit_WS1beoF"
      },
      "source": [
        "# Split data between X (features) and y (target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NykhIadWbolx"
      },
      "source": [
        "We can split features (`X`) and target (`y`) using the `train_test_split()` function from `sklearn.model_selection` — the arguments it takes are:\n",
        "\n",
        "- `X`: The features \n",
        "- `y`: The target variable\n",
        "- `test_size`: Size of the test set here `0.25`\n",
        "- `random_state`: Takes in any number and allows to reproduce same split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-IbMYPRbg4L"
      },
      "source": [
        "# Import train_test_split\n",
        "\n",
        "\n",
        "# Split data into train test splits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXDndg5fxU10"
      },
      "source": [
        "\n",
        "\n",
        "**Continuous or numeric data**\n",
        "\n",
        "Many machine learning models make assumptions about the distribution of numeric features when modeling (most commonly data is assumed to be normally distributed). Also, many numeric columns have different scales _(e.g. Age vs Salary)_. \n",
        "\n",
        "A common way to process numeric columns is through **Standardization** - where we substract their mean and divide by their standard deviation so that their mean becomes centered around 0 and have a standard deviation of 1 :\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\large{x_{scaled} = \\frac{x - mean}{std}}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/adelnehme/data-upskilling-learning-club-IV/blob/master/assets/standard_scaler.gif?raw=true?resize\" width=\"45%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "We can do this easily in `sklearn` by using the `StandardScaler()` function. Many operations in `sklearn` fit the following `.fit()` $\\rightarrow$ `.transform()` paradigm and `StandardScaler()` is no different:\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on data\n",
        "scaler.fit(df[my_column])\n",
        "\n",
        "# Transformed\n",
        "column_scaled = scaler.transform(df[my_column])\n",
        "\n",
        "# Replace column\n",
        "df[my_column] = column_scaled\n",
        "```\n",
        "\n",
        "However, it is very important to **first split** your data before scaling your features since we do not want to scale our data according to the distribution of both the training data and test data. Failing to do so results in **data leakage** and could lead to \"too good to be true\" results on testing data with relatively weaker results on unseen data. \n",
        "\n",
        "<font color=00AAFF>Ideally, scalers should be fit on **training data only** - and be used to transform both training and testing data.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os66Te3M0ixY"
      },
      "source": [
        "# Import StandardScaler\n",
        "\n",
        "\n",
        "# Intialize a scaler\n",
        "\n",
        "\n",
        "# Fit on training data\n",
        "\n",
        "\n",
        "# Transform training and test data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKBTS0PS0ixe"
      },
      "source": [
        "# Replace columns in training and testing data accordingly\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6gYcfFJUZxa"
      },
      "source": [
        "# See changes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf6Erb-M3xxE"
      },
      "source": [
        "**Categorical data**\n",
        "\n",
        "While categorical variables like country, marriage status and more are easily interpretable by humans - they need to be properly encoded to be understood by machine learning algorithms. We will be using dummy encoding *(highly similar to one-hot encoding)* where categorical variables are converted to binary (`1`,`0`) columns to indicate whether they have a certain value or not. Note that, dummy encoding generates `n-1` categories. Using a country example - `0` on all columns encodes it as France.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/adelnehme/data-upskilling-learning-club-IV/blob/master/assets/onehot_dummy.gif?raw=true\" width=\"80%\">\n",
        "</p>\n",
        "\n",
        "Using dummy encoding in `pandas` is actually very easy - we can use the `pd.get_dummies()` function which takes:\n",
        "\n",
        "- The DataFrame being converted.\n",
        "- `columns`: The name of the categorical columns to be converted.\n",
        "- `drop_first`: Boolean to indicate onehot encoding (`False`) or dummy encoding (`True`).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgPhMhuP0ixh"
      },
      "source": [
        "# One hot encode cat variables\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdBCAOlRWYsg"
      },
      "source": [
        "# See changes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRopXo88Ehsh"
      },
      "source": [
        "**Feature Engineering**\n",
        "\n",
        "Generating new predictive features from existing features is an important aspect of machine learning. New features could be engineered using:\n",
        "\n",
        "- Binning numeric values _(e.g. `age_category` column from `age` column)._\n",
        "- Interaction of 2 columns _(e.g. `total_salary`/`tenure`)._\n",
        "- Features from domain knowledge.\n",
        "\n",
        "We learned while visualizing categorical columns that being subscribed to `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, and `TechSupport` tend to drive less churn. Let's visualize this further with a new feature called `in_ecosystem` which counts the number of services a given customer is subscribed to.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unCbSHWjGQcY"
      },
      "source": [
        "# Service columns\n",
        "service_columns = ['OnlineSecurity_Yes', 'OnlineBackup_Yes', 'DeviceProtection_Yes', 'TechSupport_Yes']\n",
        "\n",
        "# Create in_ecosystem column\n",
        "\n",
        "\n",
        "# Create feature that is 1 if 2 or more services subscribed, 0 otherwise\n",
        "\n",
        "# Apply the same on test_X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAv2lbLRWV-c"
      },
      "source": [
        "# See changes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhtT-Dvu6q5p"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 3</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISvRUbPh0iyC"
      },
      "source": [
        "## **Modeling**\n",
        "\n",
        "Most machine learning models for classification aim at creating a decision boundary between data points to generate predictions. For example, here is a decision line where the target variable is whether tumor is benign or cancerous based on tumor height and width:\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/adelnehme/data-upskilling-learning-club-IV/blob/master/assets/decision_boundary.gif?raw=true?\" width=\"50%\">\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "#### **Using K-Nearest Neighbors to Generate Predictions**\n",
        "\n",
        "The K-Nearest Neighbor tries to find the label of unseen data by choosing the label of the `K` closest points to it. Using our cancerous/benign tumour example, K-Nearest Neighbor would behave like this:\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/adelnehme/data-upskilling-learning-club-IV/blob/master/assets/knn.gif?raw=true?\" width=\"50%\">\n",
        "</p>\n",
        "\n",
        "Just like almost all algorithms on `sklearn` - the `KNeighborsClassifier()` needs to be instantiated and follows the `.fit()` $\\rightarrow$ `.predict()` paradigm as such:\n",
        "\n",
        "```\n",
        "# Import algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Instantiate it\n",
        "knn = KNeighborsClassifier(n_neighbors = k)\n",
        "\n",
        "# Fit on training data\n",
        "knn.fit(train_X, train_Y)\n",
        "\n",
        "# Create predictions\n",
        "predictions = knn.predict(test_X)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wpNKPQbLERL"
      },
      "source": [
        "# Import K-Nearest Neighbor Classifier and accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Instantiate K Nearest Neighbors with 6 neighbors\n",
        "\n",
        "\n",
        "# Fit on training data\n",
        "\n",
        "# Create Predictions\n",
        "\n",
        "# Calculate accuracy score on testing data\n",
        "\n",
        "# Print test accuracy score rounded to 4 decimals\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL13w1Lb63rE"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 4</h1> </center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd7VDPtxtNFt"
      },
      "source": [
        "#### **Hyperparameter Tuning**\n",
        "\n",
        "Almost all algorithms have hyperparameters that can be tuned to fine-tune their performance, and better capture the patterns in the dataset. Having a good understanding and intuition of how algorithms work is essential to fully utilize hyperparameter tuning for the purposes of improving model performance and testing different modeling strategies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3FqjSV0t-CG"
      },
      "source": [
        "**Tuning the number of neighbors**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uv9WCD40iyV"
      },
      "source": [
        "# Instantiate K Nearest Neighbors with 8 neighbors\n",
        "\n",
        "\n",
        "# Fit on training data\n",
        "\n",
        "# Create Predictions\n",
        "\n",
        "# Calculate accuracy score on testing data\n",
        "\n",
        "# Print test accuracy score rounded to 4 decimals\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETWTRB7P6tfQ"
      },
      "source": [
        "---\n",
        "<center><h1> Q&A 5</h1> </center>\n",
        "\n",
        "---"
      ]
    }
  ]
}
